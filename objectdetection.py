# -*- coding: utf-8 -*-
"""ObjectDetection.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1vLGytn76GwiULIeiaYWxeWf4_RTPw-i4

# Accessing the data
"""

!wget http://host.robots.ox.ac.uk/pascal/VOC/voc2012/VOCtrainval_11-May-2012.tar
!tar -xf VOCtrainval_11-May-2012.tar

!pip install opencv-python matplotlib xmltodict

import torch
import torchvision
import torchvision.transforms as transforms
from torchvision.transforms import functional as F

import cv2
import os
import matplotlib.pyplot as plt
import random

import xml.etree.ElementTree as ET
from google.colab.patches import cv2_imshow

random.seed(123)

# Directory paths
IMAGE_DIR = 'VOCdevkit/VOC2012/JPEGImages/'
ANNOTATION_DIR = 'VOCdevkit/VOC2012/Annotations/'

def parse_annotation(annotation_path):
    """Parses an XML annotation file and returns the bounding boxes and their labels."""
    tree = ET.parse(annotation_path)
    root = tree.getroot()

    boxes = []
    labels = []

    for obj in root.findall('object'):
        label = obj.find('name').text
        xmlbox = obj.find('bndbox')
        box = [int(xmlbox.find('xmin').text), int(xmlbox.find('ymin').text),
               int(xmlbox.find('xmax').text), int(xmlbox.find('ymax').text)]

        boxes.append(box)
        labels.append(label)

    return boxes, labels

def plot_image_with_boxes(img_path, boxes, labels):
    """Displays an image along with its bounding boxes."""
    img = cv2.imread(img_path)
    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)

    for (xmin, ymin, xmax, ymax), label in zip(boxes, labels):
        cv2.rectangle(img, (xmin, ymin), (xmax, ymax), (0, 255, 0), 1)
        cv2.putText(img, label, (xmin, ymin-10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1)

    plt.imshow(img)
    plt.axis('off')
    plt.show()

# Get list of all images and shuffle them
all_images = os.listdir(IMAGE_DIR)
random.shuffle(all_images)

# Display first few images with annotations
for img_name in all_images[10:15]:
    annotation_path = os.path.join(ANNOTATION_DIR, img_name.split('.')[0] + '.xml')
    boxes, labels = parse_annotation(annotation_path)

    img_path = os.path.join(IMAGE_DIR, img_name)
    plot_image_with_boxes(img_path, boxes, labels)

# Same images without annotation boxes
def plot_image_with_labels(img_path, labels):
    """Displays an image along with its labels displayed below."""
    img = cv2.imread(img_path)
    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)

    plt.imshow(img)
    plt.axis('off')
    plt.title(", ".join(labels))
    plt.show()

# Display first few images with labels only (no bounding boxes)
for img_name in all_images[10:15]:
    annotation_path = os.path.join(ANNOTATION_DIR, img_name.split('.')[0] + '.xml')
    _, labels = parse_annotation(annotation_path)

    img_path = os.path.join(IMAGE_DIR, img_name)
    plot_image_with_labels(img_path, labels)

"""# Modelling for all 21 classes

Function `parse_annotation` takes the path to an XML annotation file (in Pascal VOC format) as input. It reads the file, extracts bounding box coordinates and their corresponding object labels, and returns them as two separate lists (boxes and labels).
"""

def parse_annotation(annotation_path):
    tree = ET.parse(annotation_path)
    root = tree.getroot()

    boxes = []
    labels = []
    for obj in root.findall('object'):
        label = obj.find('name').text
        bndbox = obj.find('bndbox')
        box = [
            int(float(bndbox.find('xmin').text)),
            int(float(bndbox.find('ymin').text)),
            int(float(bndbox.find('xmax').text)),
            int(float(bndbox.find('ymax').text))
        ]
        boxes.append(box)
        labels.append(label)

    return boxes, labels

"""The `VOCDetectionDataset` class is a custom PyTorch dataset tailored for the Pascal VOC dataset. It handles the reading of image and annotation data and processes them into a format suitable for object detection tasks.

`__init__:`

Initializes the dataset by reading image and annotation filenames. Optionally, it limits the dataset's size using max_num_samples. It also computes a mapping from class labels to integer indices.

`__len__:`

Returns the total number of images in the dataset.

`__getitem__:`

Given an index, it retrieves the image and its corresponding annotation, processes them, and returns the image and its target dictionary which includes boxes, labels, image_id, area, and iscrowd fields. If transforms are specified, they are applied to the image.
"""

class VOCDetectionDataset(torch.utils.data.Dataset):
    def __init__(self, root, transforms=None, max_num_samples=None):
        self.root = root
        self.transforms = transforms
        self.imgs = list(sorted(os.listdir(os.path.join(root, "JPEGImages"))))
        self.annotations = list(sorted(os.listdir(os.path.join(root, "Annotations"))))

        # Limit the dataset size if max_num_samples is specified
        if max_num_samples:
            self.imgs = self.imgs[:max_num_samples]
            self.annotations = self.annotations[:max_num_samples]

        # Compute unique labels once and store them
        all_labels = []
        for annotation in self.annotations:
            _, labels_for_annotation = parse_annotation(os.path.join(self.root, "Annotations", annotation))
            all_labels.extend(labels_for_annotation)

        self.unique_labels = ["background"] + sorted(set(all_labels))
        self.class_map = {label: idx for idx, label in enumerate(self.unique_labels)}

    def __len__(self):
        return len(self.imgs)

    def __getitem__(self, idx):
        img_path = os.path.join(self.root, "JPEGImages", self.imgs[idx])
        annotation_path = os.path.join(self.root, "Annotations", self.annotations[idx])

        img = cv2.imread(img_path)
        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)

        boxes, labels = parse_annotation(annotation_path)
        boxes = torch.as_tensor(boxes, dtype=torch.float32)

        # Use precomputed labels and class map
        labels = torch.tensor([self.class_map[label] for label in labels])

        image_id = torch.tensor([idx])
        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])
        iscrowd = torch.zeros((len(boxes),), dtype=torch.int64)

        target = {}
        target["boxes"] = boxes
        target["labels"] = labels
        target["image_id"] = image_id
        target["area"] = area
        target["iscrowd"] = iscrowd

        if self.transforms:
            img = self.transforms(img)

        return img, target

"""Next we define a basic transformation for images in the PIL format to
convert them to PyTorch tensors. We create an instance of the custom VOCDetectionDataset class, initializing it with the root directory of the Pascal VOC dataset, the previously defined transformations, and an optional parameter to limit the number of samples. In this case, it's set to use the entire dataset.

We also define a custom function `collate_fn` to handle how batches of data should be grouped together. This is particularly important for object detection tasks where images can have a varying number of bounding boxes and labels.

And, finally, we create a data loader for the initialized dataset.
"""

transforms = torchvision.transforms.Compose([
    torchvision.transforms.ToTensor()
])

dataset = VOCDetectionDataset(root="VOCdevkit/VOC2012", transforms=transforms, max_num_samples=17125) # 17125 is full size of the dataset

print(f"Total images in dataset: {len(dataset)}")

def collate_fn(batch):
    return tuple(zip(*batch))

data_loader = torch.utils.data.DataLoader(dataset, batch_size=5, shuffle=True, collate_fn=collate_fn)

"""The `ObjectDetector` class is designed for training object detection models.

Upon initialization (`__init__`), it accepts a model, a data loader, and hyperparameters for optimization (like learning rate, momentum, and weight decay). It then sets up the model to run on the GPU (if available) and initializes an optimizer for training.

The `_get_optimizer` method is a helper function to generate an optimizer for the trainable parameters of the model using Stochastic Gradient Descent (SGD).

The train method handles the actual training process for a specified number of epochs. For each batch of images and corresponding targets, it computes the loss, backpropagates the error, and updates the model's weights. It also prints out loss statistics during training.

After defining the class, the code collects all unique labels from the dataset, computes the total number of unique object classes, and initializes a pre-trained Faster R-CNN model. The last part of the code modifies the final layer of this pre-trained model to predict bounding boxes and class scores for the number of classes in the dataset, accounting for the background class as well.
"""

class ObjectDetector:
    def __init__(self, model, data_loader, lr=0.005, momentum=0.9, weight_decay=0.0005):
        self.model = model
        self.data_loader = data_loader
        self.device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')
        self.model.to(self.device)
        self.optimizer = self._get_optimizer(lr, momentum, weight_decay)

    def _get_optimizer(self, lr, momentum, weight_decay):
        params = [p for p in self.model.parameters() if p.requires_grad]
        return torch.optim.SGD(params, lr=lr, momentum=momentum, weight_decay=weight_decay)

    def train(self, num_epochs=3):
        for epoch in range(num_epochs):
            self.model.train()
            total_loss = 0.0
            for batch_num, (images, targets) in enumerate(self.data_loader):
                images = list(image.to(self.device) for image in images)
                targets = [{k: v.to(self.device) for k, v in t.items()} for t in targets]

                if not images:  # if all images in this batch are None
                    continue

                images = list(image.to(self.device) for image in images)
                targets = [{k: v.to(self.device) for k, v in t.items()} for t in targets]

                loss_dict = self.model(images, targets)
                losses = sum(loss for loss in loss_dict.values())
                total_loss += losses.item()

                self.optimizer.zero_grad()
                losses.backward()
                self.optimizer.step()

                if batch_num % 5 == 0:
                    print(f"Epoch {epoch}/{num_epochs}, Batch {batch_num}/{len(self.data_loader)}, Loss: {losses.item()}")

            print(f"Epoch {epoch}/{num_epochs}, Average Loss: {total_loss/len(self.data_loader)}")

# Collect all labels from the dataset
all_labels = []
for annotation in dataset.annotations:
    boxes, labels = parse_annotation(os.path.join(dataset.root, "Annotations", annotation))
    all_labels.extend(labels)

# Convert list of labels to a set to get unique labels and then compute its length
unique_labels = set(all_labels)
num_classes = len(unique_labels) + 1  # +1 for background

print(f"Number of unique classes: {num_classes}")

model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)
in_features = model.roi_heads.box_predictor.cls_score.in_features
model.roi_heads.box_predictor = torchvision.models.detection.faster_rcnn.FastRCNNPredictor(in_features, num_classes)

"""Training, saving, loading:"""

# Takes a long time for training!

trainer = ObjectDetector(model, data_loader)
trainer.train(num_epochs=3)

torch.save(trainer.model.state_dict(), 'model_21_class_17125_images.pth')

# Step 1: Define the model architecture
model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=False, num_classes=21)

# Step 2: Load saved weights
model.load_state_dict(torch.load('model_21_class_17125_images.pth'))

# Step 3: Switch to evaluation mode
model.eval()

if torch.cuda.is_available():
    model = model.cuda()

"""# Plotting results"""

def detect_objects_in_image(image_path):
    # Load the model
    num_classes = 21  # 20 classes + 1 background
    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=False, num_classes=num_classes)
    model.load_state_dict(torch.load('model_21_class_1000_images.pth'))
    model.eval()  # Important: set the model to evaluation mode
    model = model.to(torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu'))

    # Load and preprocess the image
    image = cv2.imread(image_path)
    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
    transform = transforms.Compose([
        transforms.ToPILImage(),
        transforms.ToTensor()
    ])
    image_tensor = transform(image).unsqueeze(0).to(torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu'))

    # Get predictions
    with torch.no_grad():
        prediction = model(image_tensor)

    # Post-process the output (e.g., apply a confidence threshold)
    confidence_threshold = 0.5
    pred_boxes = prediction[0]['boxes'][prediction[0]['scores'] > confidence_threshold].cpu().numpy()
    pred_labels = prediction[0]['labels'][prediction[0]['scores'] > confidence_threshold].cpu().numpy()

    # Create a dictionary to map label indices to class names (VOC class names)
    label_to_name = {
        0: "background",
        1: "aeroplane", 2: "bicycle", 3: "bird", 4: "boat",
        5: "bottle", 6: "bus", 7: "car", 8: "cat", 9: "chair",
        10: "cow", 11: "diningtable", 12: "dog", 13: "horse",
        14: "motorbike", 15: "person", 16: "pottedplant",
        17: "sheep", 18: "sofa", 19: "train", 20: "tvmonitor"
    }

    # Visualize the results
    for box, label in zip(pred_boxes, pred_labels):
        cv2.rectangle(image, (int(box[0]), int(box[1])), (int(box[2]), int(box[3])), (0, 255, 0), 2)
        class_name = label_to_name[label]
        cv2.putText(image, class_name, (int(box[0]), int(box[1])-10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)

    image_display = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)
    cv2_imshow(image)
    cv2.waitKey(0)
    cv2.destroyAllWindows()

# Example usage:
detect_objects_in_image('KOA_Nassau_2697x1517.jpg')
detect_objects_in_image('dog-photo.jpg')
detect_objects_in_image('dog_another.png')

detect_objects_in_image('service-pnp-ppbd-00600-00605r.jpg')
detect_objects_in_image('Hillary_Clinton_Arizona_2016_.jpg')
detect_objects_in_image('trump.jpeg')